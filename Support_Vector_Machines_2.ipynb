{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatisa2000/hw4/blob/main/Support_Vector_Machines_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_6codqFnXgi"
      },
      "source": [
        "# SVM for classification, without and with kernels\n",
        "\n",
        "In this notebook we are going to explore the use of Support Vector Machines (SVMs) for image classification. We are going to use the famous MNIST dataset, that is a dataset of handwritten digits. We get the data from mldata.org, that is a public repository for machine learning data.\n",
        "\n",
        "The dataset consists of 70,000 images of handwritten digits (i.e., 0, 1, ... 9). Each image is 28 pixels by 28 pixels and we can think of it as a vector of 28x28 = 784 numbers. Each number is an integer between 0 and 255. For each image we have the corresponding label (i.e., 0, 1, ..., 9)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7owrXabnXgn"
      },
      "outputs": [],
      "source": [
        "#load the required packages\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import sklearn\n",
        "from sklearn import svm, metrics\n",
        "from sklearn.datasets import fetch_openml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1Q2n9z_nXgr"
      },
      "source": [
        "Now let's load the dataset. 'data' contains the input, 'target' contains the label. We normalize the data by dividing each value by 255 so that each value is in [0,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zn7ErChanXgs"
      },
      "outputs": [],
      "source": [
        "#TODO: Normalize MNIST dataset and rescale the data\n",
        "# Load the MNIST dataset and normalize the features so that each value is in the range [0,1]\n",
        "mnist = fetch_openml('mnist_784', version=1)\n",
        "# Rescale the data\n",
        "X, y = mnist.data.to_numpy() / 255., mnist.target.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "permutation = np.random.permutation(X.shape[0])\n",
        "valid_indices = [index for index in permutation if index in X.columns]\n",
        "print(X.index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xVNK9clpg0s",
        "outputId": "59653acd-1475-40a6-8c9b-cf3fc95dc608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RangeIndex(start=0, stop=70000, step=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THXcR9T1nXgu"
      },
      "source": [
        "Now split into training and test. We keep 500 samples in the training set. Make sure that each label is present at least 10 times\n",
        "in training. If it is not, then keep adding permutations to the initial data until this\n",
        "happens.\n",
        "\n",
        "**IMPORTANT**: if you cannot run the SVM with 500 samples or 1000 samples (see below), try with a smaller number of samples (e.g. 200 here and 400 below)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mK2rPXy3nXgv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9831e20d-03a0-4451-8223-2353c7219397"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels and their frequencies in the training dataset: \n",
            "0: 49\n",
            "1: 53\n",
            "2: 48\n",
            "3: 51\n",
            "4: 53\n",
            "5: 42\n",
            "6: 46\n",
            "7: 52\n",
            "8: 65\n",
            "9: 41\n"
          ]
        }
      ],
      "source": [
        "# Randomly permute the data and split it into training and test sets, taking the first 500\n",
        "# data samples as training and the rest as test.\n",
        "permutation = np.random.permutation(X.shape[0])\n",
        "\n",
        "X = X[permutation]\n",
        "y = y[permutation]\n",
        "\n",
        "m_training = 500\n",
        "\n",
        "X_train, X_test = X[:m_training], X[m_training:]\n",
        "y_train, y_test = y[:m_training], y[m_training:]\n",
        "\n",
        "# Print the labels and their frequencies in the training dataset.\n",
        "print(\"Labels and their frequencies in the training dataset: \")\n",
        "unique_labels, label_counts = np.unique(y_train, return_counts=True)\n",
        "for label, count in zip(unique_labels, label_counts):\n",
        "    print(f\"{label}: {count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45ksUODunXgw"
      },
      "source": [
        "We now provide a function to print an image in a dataset, the corresponding true label, and the index of the image in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_digit(X_matrix, labels, index):\n",
        " \"\"\"Plots a digit from the X_matrix and prints the corresponding label.\n",
        " Args:\n",
        " X_matrix (numpy.ndarray): Matrix of digit images.\n",
        " labels (numpy.ndarray): Array of digit labels.\n",
        " index (int): Index of the digit to plot and print.\n",
        "\"\"\"\n",
        " print(\"INPUT:\")\n",
        " plt.imshow(\n",
        "        X_matrix[index].reshape(28,28),\n",
        "        cmap          = plt.cm.gray_r,\n",
        "        interpolation = \"nearest\"\n",
        "    )\n",
        " plt.show()\n",
        " print(\"LABEL: %s\" % labels[index])"
      ],
      "metadata": {
        "id": "TeXh2BpHrd-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEDj_SMOnXgz"
      },
      "source": [
        "As an example, let's print the 100-th image in X_train and the 40,000-th image in X_test and their true labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9USuS-ztnXg1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "outputId": "5b81ef05-7ca3-4533-c6f1-d93f81f023fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LABEL: 7\n",
            "INPUT:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LABEL: 0\n"
          ]
        }
      ],
      "source": [
        "# Plot the 100th digit in the training set\n",
        "plot_digit(X_train, y_train, 100)\n",
        "\n",
        "# Plot the 40,000th digit in the test set\n",
        "plot_digit(X_test, y_test,40000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dp9-uKtFnXg4"
      },
      "source": [
        "## Section 1\n",
        "Run SVM with cross validation to pick a kernel and values of parameters. Use a 5-fold cross-validation to pick the best kernel and choice of parameters. We provide some potential choice for parameters, but change the grid if needed (e.g., it takes too long). For the SVM for classification use SVC from sklearn.svm; for the grid search we suggest you use GridSearchCV from sklearn.model_selection, but you can implement your own cross-validation for model selection if you prefer.\n",
        "\n",
        "Print the best parameters used as well as the score obtained by the best model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Support Vector Classifier (SVC) and GridSearchCV modules\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Set the parameters for the linear SVM\n",
        "parameters = {'C': [1, 10, 100]}\n",
        "\n",
        "# Create a linear SVM object\n",
        "linear_SVM = SVC(kernel='linear')\n",
        "\n",
        "# TO DO: Find the best model using 5-fold cross-validation and train it using all the training data\n",
        "grid_search = GridSearchCV(linear_SVM,parameters, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_svm_classifier = grid_search.best_estimator_\n",
        "best_svm_classifier.fit(X, y)\n",
        "predicted= best_svm_classifier.predict(X_test)\n",
        "accuracy = metrics.accuracy_score(y_test,predicted)\n",
        "# Print the results for the linear kernel\n",
        "print('RESULTS FOR LINEAR KERNEL\\n')\n",
        "print(\"accuracy:\", accuracy)\n",
        "print(\"Best parameters set found:\")\n",
        "# TO DO\n",
        "print(grid_search.best_params_)\n",
        "print(\"Score with best parameters:\")\n",
        "# TO DO\n",
        "print(grid_search.best_score_)\n",
        "print(\"\\nAll scores on the grid:\")\n",
        "# TO DO\n",
        "means = grid_search.cv_results_['mean_test_score']\n",
        "stds = grid_search.cv_results_['std_test_score']\n",
        "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
        "    print(f\"Mean: {mean}, Std: {std}, Parameters: {params}\")\n",
        "\n",
        "\n",
        "# Set the parameters for the polynomial kernel with degree 2\n",
        "parameters = {'C': [1, 10, 100], 'gamma': [0.01, 0.1, 1.]}\n",
        "# Create a polynomial kernel with degree 2 SVM object\n",
        "poly2_SVM = SVC(kernel='poly', degree=2)\n",
        "\n",
        "# TO DO: Find the best model using 5-fold cross-validation and train it using all the training data\n",
        "grid_search = GridSearchCV(poly2_SVM,parameters, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_svm_classifier = grid_search.best_estimator_\n",
        "best_svm_classifier.fit(X, y)\n",
        "predicted= best_svm_classifier.predict(X_test)\n",
        "accuracy = metrics.accuracy_score(y_test,predicted)\n",
        "\n",
        "# Print the results for the polynomial kernel with degree 2\n",
        "print('\\nRESULTS FOR POLY DEGREE=2 KERNEL\\n')\n",
        "print(\"accuracy:\", accuracy)\n",
        "print(\"Best parameters set found:\")\n",
        "# TO DO\n",
        "print(grid_search.best_params_)\n",
        "print(\"Score with best parameters:\")\n",
        "# TO DO\n",
        "print(grid_search.best_score_)\n",
        "print(\"\\nAll scores on the grid:\")\n",
        "# TO DO\n",
        "means = grid_search.cv_results_['mean_test_score']\n",
        "stds = grid_search.cv_results_['std_test_score']\n",
        "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
        "    print(f\"Mean: {mean}, Std: {std}, Parameters: {params}\")\n",
        "\n",
        "\n",
        "# Set the parameters for the RBF kernel\n",
        "parameters = {'C': [1, 10, 100], 'gamma': [0.01, 0.1, 1.]}\n",
        "\n",
        "# Create an RBF kernel SVM object\n",
        "rbf_SVM = SVC(kernel='rbf')\n",
        "\n",
        "# TO DO: Find the best model using 5-fold cross-validation and train it using all the training data\n",
        "grid_search = GridSearchCV(rbf_SVM,parameters, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_svm_classifier = grid_search.best_estimator_\n",
        "best_svm_classifier.fit(X, y)\n",
        "predicted= best_svm_classifier.predict(X_test)\n",
        "accuracy = metrics.accuracy_score(y_test,predicted)\n",
        "\n",
        "# Print the results for the RBF kernel\n",
        "print('\\nRESULTS FOR RBF KERNEL\\n')\n",
        "print(\"accuracy:\", accuracy)\n",
        "print(\"Best parameters set found:\")\n",
        "# TO DO\n",
        "print(grid_search.best_params_)\n",
        "print(\"Score with best parameters:\")\n",
        "# TO DO\n",
        "print(grid_search.best_score_)\n",
        "print(\"\\nAll scores on the grid:\")\n",
        "# TO DO\n",
        "means = grid_search.cv_results_['mean_test_score']\n",
        "stds = grid_search.cv_results_['std_test_score']\n",
        "for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n",
        "    print(f\"Mean: {mean}, Std: {std}, Parameters: {params}\")\n"
      ],
      "metadata": {
        "id": "49FHb1CKuSa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bec8d042-2fe9-49e3-dced-5a8f09805e82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RESULTS FOR LINEAR KERNEL\n",
            "\n",
            "accuracy: 0.9691798561151079\n",
            "Best parameters set found:\n",
            "{'C': 1}\n",
            "Score with best parameters:\n",
            "0.8480000000000001\n",
            "\n",
            "All scores on the grid:\n",
            "Mean: 0.8480000000000001, Std: 0.04445222154178575, Parameters: {'C': 1}\n",
            "Mean: 0.8480000000000001, Std: 0.04445222154178575, Parameters: {'C': 10}\n",
            "Mean: 0.8480000000000001, Std: 0.04445222154178575, Parameters: {'C': 100}\n",
            "\n",
            "RESULTS FOR POLY DEGREE=2 KERNEL\n",
            "\n",
            "accuracy: 0.9986762589928058\n",
            "Best parameters set found:\n",
            "{'C': 10, 'gamma': 0.01}\n",
            "Score with best parameters:\n",
            "0.868\n",
            "\n",
            "All scores on the grid:\n",
            "Mean: 0.812, Std: 0.036551333764994115, Parameters: {'C': 1, 'gamma': 0.01}\n",
            "Mean: 0.8640000000000001, Std: 0.03440930106817049, Parameters: {'C': 1, 'gamma': 0.1}\n",
            "Mean: 0.8640000000000001, Std: 0.03440930106817049, Parameters: {'C': 1, 'gamma': 1.0}\n",
            "Mean: 0.868, Std: 0.03249615361854383, Parameters: {'C': 10, 'gamma': 0.01}\n",
            "Mean: 0.8640000000000001, Std: 0.03440930106817049, Parameters: {'C': 10, 'gamma': 0.1}\n",
            "Mean: 0.8640000000000001, Std: 0.03440930106817049, Parameters: {'C': 10, 'gamma': 1.0}\n",
            "Mean: 0.8640000000000001, Std: 0.03440930106817049, Parameters: {'C': 100, 'gamma': 0.01}\n",
            "Mean: 0.8640000000000001, Std: 0.03440930106817049, Parameters: {'C': 100, 'gamma': 0.1}\n",
            "Mean: 0.8640000000000001, Std: 0.03440930106817049, Parameters: {'C': 100, 'gamma': 1.0}\n",
            "\n",
            "RESULTS FOR RBF KERNEL\n",
            "\n",
            "accuracy: 0.9995107913669065\n",
            "Best parameters set found:\n",
            "{'C': 10, 'gamma': 0.01}\n",
            "Score with best parameters:\n",
            "0.89\n",
            "\n",
            "All scores on the grid:\n",
            "Mean: 0.8779999999999999, Std: 0.029257477676655614, Parameters: {'C': 1, 'gamma': 0.01}\n",
            "Mean: 0.316, Std: 0.035552777669262355, Parameters: {'C': 1, 'gamma': 0.1}\n",
            "Mean: 0.13, Std: 0.0, Parameters: {'C': 1, 'gamma': 1.0}\n",
            "Mean: 0.89, Std: 0.05176871642217912, Parameters: {'C': 10, 'gamma': 0.01}\n",
            "Mean: 0.368, Std: 0.03969886648255841, Parameters: {'C': 10, 'gamma': 0.1}\n",
            "Mean: 0.13, Std: 0.0, Parameters: {'C': 10, 'gamma': 1.0}\n",
            "Mean: 0.89, Std: 0.05176871642217912, Parameters: {'C': 100, 'gamma': 0.01}\n",
            "Mean: 0.368, Std: 0.03969886648255841, Parameters: {'C': 100, 'gamma': 0.1}\n",
            "Mean: 0.13, Std: 0.0, Parameters: {'C': 100, 'gamma': 1.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sECvVFNvnXg6"
      },
      "source": [
        "## Section 2\n",
        "For the \"best\" SVM kernel and choice of parameters from above, train the model on the entire training set and measure the training error. Also make predictions on the test set and measure the test error. Print the training and the test error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sk0mqc0QnXg6"
      },
      "outputs": [],
      "source": [
        "# Get the best SVM model from cross-validation\n",
        "best_SVM = # TO DO\n",
        "\n",
        "# Fit the model on the entire training set\n",
        "best_SVM.fit(X_train, y_train)\n",
        "\n",
        "# Get the training and test error\n",
        "training_error = 1. - best_SVM.score(X_train, y_train)\n",
        "test_error = 1. - best_SVM.score(X_test, y_test)\n",
        "\n",
        "# Print the training and test error for the best SVM model\n",
        "print(\"Best SVM training error: %f\" % training_error)\n",
        "print(\"Best SVM test error: %f\" % test_error)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nMoD65fnXg6"
      },
      "source": [
        "## Use logistic regression for comparison\n",
        "\n",
        "## Section 3\n",
        "\n",
        "Just for comparison let's also use logistic regression, first with the default values of the parameter for regularization and then with cross-validation to fix the value of the parameter. For cross validation, use 5-fold cross validation and the default values of the regularization parameters for the function linear_model.LogisticRegressionCV(...)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "# Create a logistic regression object\n",
        "lr = linear_model.LogisticRegression()\n",
        "\n",
        "# TODO: Fit the model on the training data\n",
        "# ADD CODE\n",
        "lr.fit(X_train,y_train)\n",
        "\n",
        "# Compute the training and test error for the logistic regression model\n",
        "cv_scores = cross_val_score(lr,X_train,y_train,cv=5)\n",
        "training_error =cv_scores.mean()\n",
        "test_error = 1 - lr.score(X_test, y_test)\n",
        "\n",
        "print(\"Best logistic regression training error: %f\" % training_error)\n",
        "print(\"Best logistic regression test error: %f\" % test_error)\n",
        "\n",
        "# Use logistic regression with 5-fold cross-validation\n",
        "# You can use linear_model.LogisticRegressionCV\n",
        "# Use 5-fold cross-validation to find the best choice of the parameter, then train\n",
        "# the model on the entire training set\n",
        "\n",
        "lr_cv =LogisticRegressionCV(cv=5)\n",
        "lr_cv.fit(X_train, y_train)\n",
        "best_param = lr_cv.C_\n",
        "best_lr = LogisticRegressionCV(cv=5, Cs=[best_param])\n",
        "best_lr.fit(X_train, y_train)\n",
        "test_accuracy = best_lr.score(X_test, y_test)\n",
        "training_error_cv = 1 - lr_cv.score(X_train, y_train)\n",
        "test_error_cv= 1 - lr_cv.score(X_test, y_test)\n",
        "print(\"Best logistic regression training error: %f\" % training_error_cv)\n",
        "print(\"Best logistic regression test error: %f\" % test_error_cv)\n"
      ],
      "metadata": {
        "id": "lHBdJ76QtwDB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "feffdf68-efe1-4a39-ee3d-941a83bdcdec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best logistic regression training error: 0.856000\n",
            "Best logistic regression test error: 0.148820\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-f2ced0077dfa>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mbest_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mbest_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegressionCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbest_param\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mbest_lr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_lr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mtraining_error_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlr_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1867\u001b[0m             \u001b[0mprefer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"processes\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1869\u001b[0;31m         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n\u001b[0m\u001b[1;32m   1870\u001b[0m             path_func(\n\u001b[1;32m   1871\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1083\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1085\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1086\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    899\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    289\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_log_reg_scoring_path\u001b[0;34m(X, y, train, test, pos_class, Cs, scoring, fit_intercept, max_iter, tol, class_weight, verbose, solver, penalty, dual, intercept_scaling, multi_class, random_state, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0msample_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m     coefs, Cs, n_iter = _logistic_regression_path(\n\u001b[0m\u001b[1;32m    724\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             ]\n\u001b[0;32m--> 450\u001b[0;31m             opt_res = optimize.minimize(\n\u001b[0m\u001b[1;32m    451\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                 \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    694\u001b[0m                                  **options)\n\u001b[1;32m    695\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m         res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[1;32m    697\u001b[0m                                callback=callback, **options)\n\u001b[1;32m    698\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_lbfgsb_py.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    303\u001b[0m             \u001b[0miprint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m     sf = _prepare_scalar_function(fun, x0, jac=jac, args=args, epsilon=eps,\n\u001b[0m\u001b[1;32m    306\u001b[0m                                   \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_bounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                                   finite_diff_rel_step=finite_diff_rel_step)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;31m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;31m# calculation reduces overall function evaluations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m     sf = ScalarFunction(fun, x0, args, grad, hess,\n\u001b[0m\u001b[1;32m    333\u001b[0m                         finite_diff_rel_step, bounds, epsilon=epsilon)\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;31m# Gradient evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0;31m# Overwriting results in undefined behaviour because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;31m# fun(self.x) will change self.x, with the two no longer linked.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0;31m# Make sure the function returns a true scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;34m\"\"\" returns the function value \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_linear_loss.py\u001b[0m in \u001b[0;36mloss_gradient\u001b[0;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_dof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"F\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;31m# grad_pointwise.shape = (n_samples, n_classes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_pointwise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml2_reg_strength\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_pointwise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,) (10,784) "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1zBeUFYnXg8"
      },
      "source": [
        "## Section 4\n",
        "Compare and comment the results from SVM and logistic regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjEGnvoynXg8"
      },
      "source": [
        "## Section 5\n",
        "Write the code that finds and plots a digit that is missclassified by logistic regression (optimized for the regularization parameter) and correctly classified by the \"best\" SVM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "md8_fNFxnXg8"
      },
      "outputs": [],
      "source": [
        "# ADD CODE\n",
        "y_pred_lr = best_lr.predict(X_test)\n",
        "misclassified_i = [i for i in range(len(y_test)) if y_test[i] != y_pred_lr[i]]\n",
        "misclassified_di = misclassified_i[0]\n",
        "misclassified_dim = X_test[misclassified_di].reshape(8, 8)\n",
        "\n",
        "# Find correctly classified digit by optimized SVM\n",
        "y_pred_svm = best_svm.predict(X_test)\n",
        "correctly_i = [i for i in range(len(y_test)) if y_test[i] == y_pred_svm[i]]\n",
        "correctly_di = correctly_i[0]\n",
        "correctly_dim = X_test[correctly_di].reshape(8, 8)\n",
        "\n",
        "# Plot the misclassified and correctly classified digits\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(misclassified_dim, cmap='gray')\n",
        "plt.title('Misclassified Digit (Logistic Regression)')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(correctly_dim, cmap='gray')\n",
        "plt.title('Correctly Classified Digit (SVM)')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrWB8rZvnXg9"
      },
      "source": [
        "## More data\n",
        "Now let's do the same but using 1000 data points for training.\n",
        "\n",
        "## Section 6\n",
        "Repeat the entire analysis above using 1000 samples. Of course you can copy the code from above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gkWCjdVnXg9"
      },
      "source": [
        "## Section 7\n",
        "Compare and comment on the differences with the results above."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}